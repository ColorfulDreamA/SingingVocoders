# preprocessing
base_config:
  - configs/base_hifi.yaml
raw_data_dir: []
binary_data_dir: null
binarization_args:
  num_workers: 8
  shuffle: true

DataIndexPath: data
valid_set_name: valid
train_set_name: train


volume_aug: true
volume_aug_prob: 0.5


mel_vmin: -6. #-6.
mel_vmax: 1.5


audio_sample_rate: 44100
audio_num_mel_bins: 128
hop_size: 512            # Hop size.
fft_size: 2048           # FFT size.
win_size: 2048           # FFT size.
fmin: 40
fmax: 16000
fmax_for_loss: null
crop_mel_frames: 32



# global constants


# neural networks


#model_cls: training.nsf_HiFigan_task.nsf_HiFigan
model_args:
  upsample_rates: [ 8, 8, 2, 2, 2 ]
  upsample_kernel_sizes: [ 16,16, 4, 4, 4 ]
  upsample_initial_channel: 512
  resblock_kernel_sizes: [ 3,7,11 ]
  resblock_dilation_sizes: [ [ 1,3,5 ], [ 1,3,5 ], [ 1,3,5 ] ]
  discriminator_periods: [ 3, 5, 7, 11, 17, 23, 37 ]
  resblock: "1"

# training

task_cls: training.nsf_HiFigan_task.nsf_HiFigan


#sort_by_len: true
#optimizer_args:
#  optimizer_cls: torch.optim.AdamW
#  lr: 0.0001
#  beta1: 0.9
#  beta2: 0.98
#  weight_decay: 0
#lab_aux_loss: 0.5
discriminate_optimizer_args:
  optimizer_cls: torch.optim.AdamW
  lr: 0.00001
  beta1: 0.9
  beta2: 0.98
  weight_decay: 0

generater_optimizer_args:
  optimizer_cls: torch.optim.AdamW
  lr: 0.00001
  beta1: 0.9
  beta2: 0.98
  weight_decay: 0

lr_scheduler_args:
  scheduler_cls: lr_scheduler.scheduler.WarmupLR
  warmup_steps: 5000
  min_lr: 0.00001

clip_grad_norm: 1
accumulate_grad_batches: 1
sampler_frame_count_grid: 6
ds_workers: 4
dataloader_prefetch_factor: 2

batch_size: 6



num_valid_plots: 100
log_interval: 100
num_sanity_val_steps: 5  # steps of validation at the beginning
val_check_interval: 2000
num_ckpt_keep: 5
max_updates: 100000
permanent_ckpt_start: 200000
permanent_ckpt_interval: 40000

###########
# pytorch lightning
# Read https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api for possible values
###########
pl_trainer_accelerator: 'auto'
pl_trainer_devices: 'auto'
pl_trainer_precision: '32-true'
#pl_trainer_precision: 'bf16' #please do not use bf 16
pl_trainer_num_nodes: 1
pl_trainer_strategy: 'auto'
ddp_backend: 'nccl' # choose from 'gloo', 'nccl', 'nccl_no_p2p'
seed: 114514

###########
# finetune
###########

finetune_enabled: true
finetune_ckpt_path: hifi.ckpt
finetune_ignored_params: []
finetune_strict_shapes: true

freezing_enabled: false
frozen_params: []
